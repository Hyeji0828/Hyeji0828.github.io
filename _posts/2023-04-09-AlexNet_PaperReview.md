---
layout: posts
title: "[AlexNet] ImageNet Classification with Deep Convolutional Neural Networks 리뷰"
date: 2023-04-09
category: PaperReview
---

# [AlexNet 논문](https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf) 리뷰


- Deep Convolutional Neural Networks vs CNN?

: 기본적으로 같은 말이다. 굳이 구분하자면 Deep CNN은 레이어가 더 많고 복잡한 학습을 하는 경우라고 나눌 수 있다.

## 개요

- 데이터 : 120만 개의 고해상도 이미지
- 분류 클래스 : 1000개
- 뉴런 65만 개
- 파라미터 6천만 개

## 1. 소개

데이터셋과 GPU의 기술적인 향상 덕분에 레이블이 지정된 고해상도 이미지 데이터셋으로 학습할 수 있게 되었다. 그리고 네트워크를 개선하고 효과적으로 과적합을 방지해 성능을 향상시켰다. GPU와 메모리의 한계로 네트워크 크기가 제한되었지만 추후 향상된 GPU와 더 많은 데이터셋이 등장한다면 결과를 개선시킬 수 있을 것이다.

- Top1 error : 최고 예측 확률이 클래스를 맞추면 0, 못 맞추면 100인 에러
- Top5 error : 상위 예측 확률의 5개 안에 정답이 있으면 0, 없으면 100

## 2. 데이터셋

256x256의 RGB 3채널의 정사각형 이미지 데이터를 사용했다. (→ 지금은 256x256도 고해상도는 아닌데 당시 2012년 기준으로는 고해상도 였나봄)

120만 개의 Training 데이터, 5만 개의 Validation 데이터, 15만 개의 Testing 데이터

## 3. Architecture 구조

컨볼루션 레이어 x 5 + FC(Fully Connected Layer) x 3

### 3.1 ReLU Nonlinearity

기존의 Tanh, Sigmoid 함수 대신  ReLU 함수를 사용했다. 잘 작동되는 tanh(x)는 과적합 방지를 주요 목적으로 했기 때문에 큰 모델과 큰 데이터셋에는 더 빠르게 적합할 수 있는 ReLU가 더 적당하다고 보았다. 

**→ ReLU가 기존에 비해 어떤 식으로 포화를 방지할까? sigmoid, tanh와 비교해서 뭐가 좋은걸까?**

- Sigmoid : 0이나 1에 가까운 값을 가지면 기울기가 0에 가까워져서 기울기 소실 문제가 생긴다. exponential 연산이 오래 걸린다. 항상 양수값만 나와서 모든 W값이 같은 방향으로 움직인다.
- tanh : sigmoid의 문제를 일부 해결한다. -1과 1사이 값을 가지지만 여전히 값이 커지면 (뉴런 포화) 기울기 소실문제가 발생한다.
- ReLU : 양수일 때 기울기 소실문제가 발생하지 않고 계산도 빠르다. AlexNet 처음 사용

[자세한 설명](https://velog.io/@guide333/풀잎스쿨-CS231n-6강-Training-Neural-Networks-Part-1#sigmoid-함수)

### 3.2 멀티 GPU 학습

뉴런을 반으로 나눠 두 개의 GPU에 각각 배치해 분산 처리하였다. 그리고 특정 레이어에서만 GPU간 통신을 수행했다. 이런 연결성 패턴은 교차 검증과 통신 양을 조절하기 위해 사용되었다. 아키텍쳐는 columnar cnn과 유사하지만 차이점은 열들이 독립적이지 않다는 점이다. 

**-> 왜 교차를 하면서 학습하는 걸까?**

교차를 하지 않으면 레이어 앞단과 뒷단이 학습하는 feature가 달라지게 된다. 뒤로 갈수록 abstract한 feature를, 앞에서는 local한 feature(경계 등)을 추출한다. 그래서 중간 지점에서 교차를 해준 것 같다.

**→ 다른 GPU에서 받은 결과랑 내가 연산한 결과 2개는 다음 레이어에 넘어갈 때 어떻게 적용될까?**

보통은 sum하거나 concat한다. AlexNet에서는 sum을 한 것으로 보인다.

### 3.3 Local Response Normalization

ReLU는 포화를 방지하기 위해 입력 정규화를 필요로하지 않지만 Local Response Normalization이 일반화에 도움이 된다는 것을 발견했다. 

**→ Local Response Nomalization(LRN)**

ReLU이후에 값을 Normalize 하는 방법 중 하나로 지금은 잘 쓰이지 않고 Batch Normalization를 많이 쓴다고 한다.

- Local Response Normalization : CNN에서 feature map의 활성화 값을 정규화해 과적함을 방지하고 일반화 성능을 향상시키기 위해 사용된다. 특성 맵의 활성화 값이 큰 픽셀이 다른 픽셀에 비해 강한 신호를 보내는 것을 방지해 지역적인 상호 작용을 장려한다.
- 포화 : 데이터셋에 대한 학습이 충분해 더 이상 학습에 큰 의미가 없을 정도로 데이터셋이 충분히 많아졌을 때를 말한다. 즉 데이터가 포화되면 추가적인 데이터셋 확보나 모델 개선을 통해 성능을 향상시키는 것이 어려워 진다.

### 3.4 Overlapping Pooling

인접한 뉴런 그룹의 출력을 요약한다. 일반적으로 중첩 풀링을 사용하는 모델이 과적합하기 조금 더 어렵다.

s = z : 전통적인 local pooling. 

- Local Pooling : 데이터의 각 위치에 대해 독립적으로 적용되는 연산. 윈도우 사이즈와 스트라이드 값(s)로 정의한다. 윈도우 사이즈는 연산이 수행되는 영역의 크기이고 스트라이드는 인접한 풀링 윈도우 사이의 거리다.

s < z : overlapping pooling (중첩 풀링). 

- Overlapping Pooling : 더 많은 정보를 보존하고 작은 변화에도 민감하게 반응해 모델 성능향상을 가져올 수 있지만 파라미터 수와 연산량이 늘어나 더 많은 계산 비용이 필요하다.

<!-- ![Untitled](AlexNet)%20ImageNet%20Classification%20with%20Deep%20Convolu%20935f7b102fa1485599ff1bb390a5d570/Untitled.png) -->
![OverlappingPooling](/assets/images/Untitled.png)

- Pooling : 데이터 일부 정보를 유지하며 데이터 크기를 줄인다. 학습 파라미터 수를 줄이고 모델이 과적합되는 것을 방지하기 위해 사용된다. 보통 max pooling 과 average pooling 으로 나뉘며, max pooling 은 입력 데이터에서 가장 큰 값을 선택해 출력한다. 이를 통해 이미지 위치 변화에 상관없이 가장 중요한 특징을 유지한다. 반면 average pooling은 입력데이터의 평균 값을 계산하여 출력한다. 이를 통해 더 많은 정보를 유지하며 데이터 크기를 줄인다.

### 3.5 전반적인 구조

<!-- ![Untitled](AlexNet)%20ImageNet%20Classification%20with%20Deep%20Convolu%20935f7b102fa1485599ff1bb390a5d570/Untitled%201.png) -->
![AlexNetArchitecture](/assets/images/Untitled1.png)

총 8개의 레이어로 처음 5개는 합성곱, 나머지 3개는 Fully Connected Layer다. 마지막 FC 레이어의 출력은 1000개의 크래스 레이블에 대한 분포를 생성하는 1000-way softmax에 feeding 된다. 

- multinomial logistic regression objective (다항 로지스틱 회귀 목적 함수) : 여러 클래스 중 하나를 선택하는 다중 클래스 분류 문제를 해결하는데 사용된다. 각 클래스에 대한 확률을 모델링하고 예측값과 실제 레이블 간의 오차를 최소화한다. 각 클래스에 대해 선형 예측값을 계산하고 소프트맥스 함수를 통해 확률값으로 변환(모든 합이 1이 되도록)하고 실제 레이블 간의 교차 엔트로피 손실을 계산해 오차를 평가한다.

3번 째 합성곱 레이어 커널은 두번 째 레이어의 모든 커널 맵에 연결. Local Response Normalization 레이어는 1,2 번째 합성곱 레이어 뒤에 연결된다. Max pooling 레이어는 LRN 레이어 뒤와 5번째 합성곱 레이어 뒤에 연결된다. ReLU는 모든 합성곱 및 FC 레이어 출력에 적용된다. 1번 conv 레이어는 11x11x3 크기의 96개 커널을 사용해 (224x224x3) 이미지를 4픽셀 간격으로 필터링한다. 2번 conv 레이어는 1번 conv 레이어의 출력(+lRN + Pooling)을 입력으로 받아 (5x5x48) 크기의 256개의 커널로 필터링한다. 3,4,5 conv 레이어는 중간에 풀링이나 정규화 레이어가 없이 연결된다. 3번 conv 레이어는 (3x3x256)이고 2번 conv레이어의 (정규화, 풀링) 출력과 연결된 384개의 커널을 가지고 있다. 4번 conv 레이어는 크기가 (3x3x192)이고 5번 conv레이어는 (3x3x192)인 256개의 커널을 가진다. FC레이어는 각각 4096개의 뉴런을 가진다.

## 4. 과적합 줄이기

### 4.1 Data Augmentation

레이블 보존 변형을 사용해 데이터 과적합을 줄였다. 원본 이미지를 적은 계산으로 변형하고 디스크에 저장하지 않았다. 

1. 이미지 이동과 수평 반전 : 256x256 이미지에서 224x224의 5개 패치(가장자리와 가운데)를 추출하고 반전시켜 총 10개의 패치를 생성한 뒤 예측하고 평균을 냈다.
    
    **→ 비슷한 이미지일거 같은데 과적합에 어떻게 도움이 될까?**
    
    픽셀이 약간 이동해도 라벨은 변하지 않는다는 것과 중앙만 아니라 가장자리도 학습하기 위해서이지만 당시 Augmentation의 한계라고도 볼 수 있다.
    
    [참고(09.01 Understanding of Alexnet)](https://wikidocs.net/164787)
    
2. RGB 채널 강도 변경 : 픽셀 값 집합에서 PCA 수행. 자연 이미지에서 조명의 강도와 색상이 바뀌어도 개체 식별은 변하지 말아야 하는 상황을 구현.
- 패치 : 이미지나 비디오의 작은 영역. 데이터의 일부분. 전체 데이터의 특징을 파악하고 분석하는데 사용.


### 4.2 Dropout

은닉 뉴런의 출력을 절반의 확률로 0으로 만드는 방식으로 드롭아웃된 뉴런은 순전파와 역전파에 기여하지 않는다. 입력이 들어올 때마다 다른 아키텍쳐를 샘플링하지만 가중치는 공유한다. 뉴런 간 공생 관계를 줄이고 뉴런이 다른 뉴런에 의존할 수 없게 해 견고한 기능을 학습하도록 만든다. 테스트 시에는 모든 뉴런을 사용하되 출력에 0.5를 곱한다. 이렇게해서 생성된 예측 분포의 기하 평균을 취하는 것을 근사한다.

→ **Dropout 을 하면 왜 성능이 좋아지는걸까?**

[Dropout에 대한 아티클(5 Perspectives to Why Dropout Works So Well)](https://towardsdatascience.com/5-perspectives-to-why-dropout-works-so-well-1c10617b8028)

1. 노드를 삭제함으로써 어떤 노드가 중요한지, 그렇지 않은지를 알게된다. 좋은 노드가 삭제되면 비교적 덜 중요한 노드들이 더 좋은 결과를 내기 위해 노력하고, 나쁜 노드가 삭제되면 불필요한 노드를 삭제해 학습을 개선한다.
2. 앙상블(배깅) 효과. 매번 랜덤하게 노드가 삭제되니 매번 다른 구조를 가진다. → 앙상블과 비슷한 효과
3. 한 번에 많은 양의 파라미터가 동시에 업데이트 되기 때문에 오버피팅이 일어나기 쉽다. → 천천히 학습하게 한다.

## 5. Details of Learning

확률적 경사하강법을 사용했다. 배치 크기 = 128, 모멘텀 = 0.9, 가중치 감소=0.00005

- 모멘텀 : 경사하강법에서 사용되는 기법으로 기울기 값이 지그재그로 변할 때 더 큰 폭으로 이동하게 해 더 빠르게 수렴할 수 있게 해준다. 모멘텀을 사용하면 이동 거리가 이전에 이동한 거리와 현재 기울기 값에 따라 결정되어, 이전 기울기 값도 반영하면서 가속도와 비슷한 역할을 한다.

평균0, 표준편차 0.01인 정규분포로 가중치 값 초기화. 2,4,5 conv레이어와 fc 레이어의 bias는 1로 초기화. (→ ReLU 양수 입력을 제공해 초기 학습 가속화) 나머지 레이어의 bias는 0으로 초기화. 모든 레이어에 동일한 학습률을 적용했고 수동으로 조정했다. 사용한 휴리스틱은 현재 학습률로 검증 오류율이 개선되지 않으면 학습률을 10으로 나누는 것이었다. 0.01에서 시작해 세 번 감소되었다. 1.2백만 개 이미지를 사용해 약 90사이클을 돌았고 GTX 580 3G 2개에서 5~6일 걸렸다.

## 6. 결과

### 6.1 Qualitative Evaluations 정성 평가

- 중앙이 아닌 객체도 인식

- 합리적인 상위 레이블

- 사진의 초점이 애매한 경우 → grille, cherry 

**→ 저런 사진은 어떻게 학습해야 할까?**

색을 밝게하거나 CutMix같은 방식을 사용할 수 있다. 하지만 한 사진 내에 여러 클래스가 있다면 세팅의 문제라고 할 수 있다. 현업에서 실제로 이런 데이터들을 만나기 때문에 보통 모델링을 하기 전에 데이터를 먼저 cleaning 하는 것이 좋다고 한다.

마지막 hidden 레이어에서 유도하는 특성 활성화를 고려하는 방법도 있다. 두 이미지가 작은 유클리드 거리 분리를 가지는 activation vector(특성 활성화 벡터)를 생성하면 두 이미지가 유사하다고 판단한다고 볼 수 있다. 4096 차원의 실수 벡터 끼리의 유클리드 거리로 유사도를 측정하는 것은 비효율적이기 때문에 이 벡터들을 짧은 이진코드로 압축하는 오토인코더를 훈련시키면 효율적으로 만들 수 있다. 이는 유사한 엑지 패턴을 가진 이미지를 검색하는 경향이 있다.

**→ 특성 활성화 벡터?** 

마지막 dense layer에서 추출한 벡터로 모델이 학습한 최종 벡터이다. 유사한 클래스는 유사한 유클리드 벡터를 가질 것이라 예상한다.

## 7. Discussion 고찰

이 연구를 통해 Deep CNN이 순전파 학습만으로 어려운 데이터셋에서 기록적인 성과를 달성했다는 것을 보여주었다. 신경망에서 단일 컨볼루션 레이어를 제거했을 때 성능이 떨어지는 것을 보면 우리의 결과에 깊이가 중요한 역할을 했다는 것을 알 수 있다. 컴퓨팅 파워를 확보한다면 비지도 사전 훈련 또한 도움이 될 것으로 예상된다.

---
